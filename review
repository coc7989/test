Ahh, got it üëç ‚Äî thanks for clarifying!
So in your environment, users don‚Äôt provide an expected runtime (walltime) ‚Äî jobs just run until they finish.

That changes the angle a bit:

The pain point isn‚Äôt wrong user estimates.

The opportunity is: if the system knows in advance how long jobs will run, it can schedule smarter, predict queue times better, and optimize resource usage.


Let me reframe the hackathon draft accordingly üëá


---

Hackathon Proposal: Job Runtime Prediction for IBM LSF

Problem Statement

In our current cluster setup, users submit jobs without specifying expected runtime. Jobs run until they finish, which makes it hard for the scheduler to plan efficiently. Without knowing runtimes in advance, the system cannot make smart scheduling decisions, and users have no visibility into how long their jobs might wait or run.

If we can predict the runtime of jobs automatically using AI/ML, the scheduler can plan better, and users can get clearer expectations.


---

Current Challenges

Scheduler has no knowledge of job duration until it completes.

Hard to optimize scheduling and backfilling, leading to idle resources.

Users don‚Äôt know in advance how long their jobs will take.

Queue wait times are unpredictable, causing frustration.

Long jobs may block resources that could be used by shorter ones.



---

Business Impact / Outcome

Better scheduling & backfilling ‚Üí improved overall cluster utilization.

Reduced waiting times ‚Üí faster turnaround for jobs.

Predictable experience for users ‚Üí estimated runtime and completion time.

Supports smarter policies ‚Üí SLA-based scheduling, deadline-aware queues, and energy-efficient job placement.

Improved ROI ‚Üí more work done on the same hardware.



---

Project Requirements

Access to job history (job start time, end time, resources used, user metadata).

ML/AI pipeline to predict job runtime from job metadata (scripts, user history, job type, resources requested).

Simple API or integration with LSF to provide runtime estimates at submission.

Dashboard to show predicted vs. actual runtimes for transparency.



---

Estimated Efforts

Week 1‚Äì2: Collect and preprocess job history data.

Week 3‚Äì4: Build baseline ML/AI model for runtime prediction.

Week 5: Integrate with job submission and scheduling workflow (prototype).

Week 6: Validation, user feedback, and final demo.


Total: ~6 weeks for a working prototype.


---

üëâ Do you want me to also draft a ‚Äúfuture possibilities‚Äù section (like extending this to cost prediction, cloud bursting, SLA queues), or should I keep it focused only on the hackathon prototype?

